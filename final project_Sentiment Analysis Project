import pandas as pd
import numpy as np
import matplotlib.pyplot as plt  
import seaborn as sns 

# Load the healthcare data from CSV
HC_Review=pd.read_csv(r"C:\Users\Admin\Documents\Downloads\healthcare_reviews.csv")
HC_Review.head()

**DATASET**:
**Data Cleaning :**
HC_Review.isnull().sum()

HC_Review.shape

#Handling Missing Value
#If a small percentage of data is missing (e.g., 10%), it might be acceptable to delete those rows
df=HC_Review.dropna()
df.shape

# 1) Lowering of data
df['Review_Text']=df['Review_Text'].str.lower()

# **Define keywords for sentiment analysis**
positive_keywords = ['great', 'excellent', 'highly recommended', 'satisfied', 'caring', 'attentive', 'couldn\'t be happier', 'very satisfied']
negative_keywords = ['bad experience', 'avoid', 'terrible', 'disappointing', 'would not recommend', 'won\'t be coming back']
# Defining the Data Cleaning Function
def clean_data(row):
    review = row['Review_Text'].lower() # Converts the review text to lowercase for consistent keyword matching
    rating = row['Rating']
    #Counts the occurrences of positive and negative keywords in the review text.
    positive_score = sum([1 for keyword in positive_keywords if keyword in review])
    negative_score = sum([1 for keyword in negative_keywords if keyword in review]) 
    
    if positive_score > negative_score and rating < 4:
        return 5 #If positive sentiment (keywords) is higher and the original rating is below 4, the rating is corrected to 5.

    elif negative_score > positive_score and rating > 2:
        return 1 #If negative sentiment (keywords) is higher and the original rating is above 2, the rating is corrected to 1.

    else:
        return rating  #If thereâ€™s no clear sentiment or no conflict between sentiment and rating, the original rating is retained.

# Apply the function to clean the ratings
df['cleaned_rating'] = df.apply(clean_data, axis=1)

# Check the cleaned data
df.head()

#Assigning Sentiment Based on Cleaned Ratings
df.loc[df['cleaned_rating'] >3, 'Sentiment'] = 'positive'
df.loc[df['cleaned_rating'] ==3, 'Sentiment'] = 'neutral'
df.loc[df['cleaned_rating'] <3, 'Sentiment'] = 'negative'

#Assigning Numerical Sentiment Ratings Based on Cleaned Ratings
df.loc[df['cleaned_rating'] >3, 'Sentiment_rating'] = '1'
df.loc[df['cleaned_rating'] ==3, 'Sentiment_rating'] = '0'
df.loc[df['cleaned_rating'] <3, 'Sentiment_rating'] = '-1'

# import libraries for Data Cleaning
import nltk
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords # Import the NLTK library and download the stop words data
nltk.download('stopwords')
import string  
from nltk.stem import WordNetLemmatizer
nltk.download('wordnet')
nltk.download('punkt')

# 2) Tokenization of data
#Tokenize text data in a DataFrame
df['tokenized_reviews'] = df.apply(lambda row: nltk.word_tokenize(row['Review_Text']), axis=1)
# Define the list of English stop words and custom words to retain
english_stopwords = stopwords.words('english')
custom_words_to_retain = ['nor', 'not', "don't", ...]  # ... indicates that you can add more custom words as needed.

# Define a function to remove stop words
def remove_stop_words(words_list):
    return [t for t in words_list if t not in english_stopwords or t in custom_words_to_retain]
df['stopwords_removed']=df.apply(lambda row:remove_stop_words(row['tokenized_reviews']),axis=1)
# 4) Punctuations Removal in data
punctuations = string.punctuation
def remove_punct(words_list):
    return [t for t in words_list if t not in punctuations]
df['punct_removed']=df.apply(lambda row:remove_punct(row['stopwords_removed']),axis=1)
# 5) Lemmatization is the process of reducing a word to its base or root form
lm=WordNetLemmatizer()
def lemmatize_list(words_list):
    return [lm.lemmatize(t) for t in words_list]
df['lemmatized']=df.apply(lambda row:lemmatize_list(row['punct_removed']),axis=1)
# 6) Convert list to string - lemmatized column
def convert_to_string(words_list):
    return " ".join(words_list)
# 6) Convert list to string - lemmatized column
def convert_to_string(words_list):
    return " ".join(words_list)
df.head()
# save DataFrame to csv
df.to_csv(r"C:\Users\Admin\Documents\Downloads\Cleaned_Dataset.csv",index=False)
#count the number of occurrences of each unique value in a Series.
df['Sentiment_rating'].value_counts()
# import libraries for Machine Learning task
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import confusion_matrix, accuracy_score,classification_report
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
# Model Building :
#splitting the dataset into training and testing sets for machine learning tasks.

X=df['cleaned'].values # X variable holds the feature data(independent variables)
y=df['Sentiment_rating'].values # y variable holds the target data(dependent variable)

X_train, X_test, y_train, y_test= train_test_split(X,y,test_size=0.2,random_state=42)

print('X_train='+str(len(X_train)))
print('X_test='+str(len(X_test)))


print('y_train='+str(len(y_train)))
print('y_test='+str(len(y_test)))
#Count Vectorizer :
## Initialize CountVectorizer with ngram_range=(1,2)
cv = CountVectorizer(ngram_range=(1,2))

# Fit and transform on training data
X_train_cv = cv.fit_transform(X_train)

# Transform test data
X_test_cv = cv.transform(X_test)

# Ensure y_train and y_test are integers
y_train=y_train.astype(int)
y_test=y_test.astype(int)


#This is useful to check the distribution of target classes to understand if there's any class imbalance.
# Print counts of labels in y_train
print("y_train, counts of label '1': {}".format(np.count_nonzero(y_train == 1))) 
print("y_train, counts of label '-1': {}".format(np.count_nonzero(y_train == -1))) 
print("y_train, counts of label '0': {}".format(np.count_nonzero(y_train == 0))) 

# Print counts of labels in y_test
print("y_test, counts of label '1': {}".format(np.count_nonzero(y_test == 1))) 
print("y_test, counts of label '-1': {}".format(np.count_nonzero(y_test == -1))) 
print("y_test, counts of label '0': {}".format(np.count_nonzero(y_test == 0))) 
### KNN CLASSIFIER
#KNN Classifier :
# Choosing the value of K
# Initialize empty lists and dictionary
K = [] # List to store values of k
training = []  # List to store training scores
test = [] # List to store test scores
scores = {} # Dictionary to store scores for each k

# Loop through different values of k from 2 to 20
for k in range(2, 21): 
    clf = KNeighborsClassifier(n_neighbors = k)     # Initialize KNN classifier with current k
    clf.fit(X_train_cv, y_train)     # Train the classifier

# Calculate training and test scores

    training_score = clf.score(X_train_cv, y_train) 
    test_score = clf.score(X_test_cv, y_test) 
    K.append(k) # Append current k value to K list
  
# Append scores to respective lists
    training.append(training_score) 
    test.append(test_score) 
    scores[k] = [training_score, test_score] # Store scores in the dictionary
    
# Print the scores for each k    
for keys, values in scores.items(): 
    print(keys, ':', values) 

# Optimal K value is 11
knnclf = KNeighborsClassifier(n_neighbors = 11) 
knnclf.fit(X_train_cv, y_train)

# Plotting K values vs. Training accuracy
plt.figure(figsize=(10, 6))
plt.plot(K, training, marker='o', linestyle='-', color='b', label='Training Accuracy')
# To add Testing accuracy to the plot
plt.plot(K, test, marker='o', linestyle='-', color='r', label='Testing Accuracy')
plt.title('KNN Classifier Accuracy vs. K Value')
plt.xlabel('K Value')
plt.ylabel('Accuracy')
plt.xticks(K)
plt.legend()
plt.grid(True)
plt.show()

y_pred=knnclf.predict(X_test_cv)
cm=confusion_matrix(y_test,y_pred)
print(cm)
print(accuracy_score(y_test,y_pred)) #computes the accuracy of the classification model.
print(classification_report(y_test,y_pred))

# Assuming X_train_cv and y_train are your training data
logistic_reg_clf=LogisticRegression()
logistic_reg_clf.fit(X_train_cv, y_train)
y_pred=logistic_reg_clf.predict(X_test_cv)
cm=confusion_matrix(y_test,y_pred)
print(cm)
print(accuracy_score(y_test,y_pred))

# Plot the confusion matrix
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, cmap='Blues', fmt='d')
plt.xlabel('Predicted labels')
plt.ylabel('True labels')
plt.title('Confusion Matrix')
plt.show()

## Exploratory Data Analysis
sentiment_counts = df['Sentiment'].value_counts()
fig = px.bar(sentiment_counts,height=400,width=700)
fig.update_layout(showlegend=False)
fig.show()

## Project Summary
#### The primary objective of this project was to predict sentiment ratings from cleaned textual data using various machine learning algorithms. I implemented and evaluated three different algorithms: 
* K-Nearest Neighbors (KNN) and Logistic Regression to determine their effectiveness in sentiment analysis tasks.

* In Data Collection and Cleaning, Ratings of the positive and negative sentiments were corrected accordingly to ensure high-quality input data. The cleaned text was used as features (X), and sentiment ratings were used as the target variable (y).
* In Dataset Splitting random state 42 was used to ensure reproducibility.
* In Evaluation Metrics, in addition to accuracy, precision, recall, and F1-score metrics were used to evaluate model performance comprehensively.

## Insights
* Null value 100 ROWs have been removed since its around 10% of dataset.
* In review text, low rating were given under 'health care provider was excellent', 'very satisfied with the service received' and 'staff was caring and attentive' respective ratings have been cleaned to have proper input data which has supported in achieving the proper accuracy score.
* same way, high rating were given under review texts: 'terrible experience' and 'bad experience with the health care provider', respevitve ratings have been cleanded to have proper input data which has also supported in achieving proper accuracy score.

## Conclusion
#### This project successfully demonstrated that KNN and Logistic Regression models are all effective for sentiment analysis, each achieving an accuracy of (78-79)%, achieving algorithm robustness which incdicates sentiment analysis problem is well defined. 
#### The results underscore the importance of data quality and feature representation in machine learning tasks.  
